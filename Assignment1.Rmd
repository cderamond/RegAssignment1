---
title: 'Assignment 1: Motor Trend report'
author: "C. Deramond"
date: "March, 2015"
output:
  html_document:
    highlight: pygments
    theme: flatly
---
# Executive summary

lalala

# Introduction

In this report we'll study the relationship between MPG and several features of vehicles. By the end of this study we hope to have answered these two questions:

1. *“Is an automatic or manual transmission better for MPG”*

2. *"Quantify the MPG difference between automatic and manual transmissions"*

All the data used in this report comes from `mtcars` dataset available on R datasets package.

# Analysis

## Exploratory

```{r , echo=FALSE, results='hide', cache=TRUE}
library(datasets)
library(ggplot2)
library(GGally)
tol_cor <- .55

yvx <- cor(mtcars)[1,]
selSet <- names(yvx[ which(abs(yvx) > tol_cor)]) # we will include only the ones with |cor| > .55
data<- mtcars[,selSet]

##grafico de pares
pairsXY <- ggpairs(
                data,
                title = "Y vs X, scatter matrix",
                upper = list(continuous = "cor"),
                lower = list(continuous = "smooth", 
                             params = c(method = "lm", fill = "red"))
)
##grafico de corr
corXY <- ggcorr(data) #+ ggtitle("Correlation matrix")
##

```

We'll start the process by discarding all variables which have a lower than `r tol_cor` correlation with our output `mpg`. This process lefts us with `r length(selSet) - 1` variables. 

A quick [scatter matrix](#g01) shows many interesting relationships that we should address:

- ` cil, vs, am ` are discrete variable variables. We take out `vs` as it is of low interest.
```{r, echo=FALSE}
data<- data[,-7]

```

        
- the correlation within the predictors is quite high, so risk of multicollinearity is there.

```{r svdChunk, echo=FALSE, cache=TRUE}
# svd
svd1 <- svd(scale(data[,-1])) #no need for Y on this
# get 3 top contributor
top3Contrib <- sort(abs(svd1$v[,2]), 
                    decreasing = T, 
                    index.return=T)$ix [1:3] +1  #straight from EDA class, we take 3 top contributors, we add one as we removed originally the Y variabl
## we now know that by taking the top 3 contributor we should explain pVarEx %
pVarEx <- sum((svd1$d^2/sum(svd1$d^2))[1:3]) #round it to 2 decimals *100 
```

A second, more refined approach, singular value decomposition: by merely taking 3 variables we ought to explain **`r paste(round(pVarEx*100,1),"%")`** of the variation. The variables that will make it to the linear model are:

```{r, echo=FALSE}
names(data[, top3Contrib])
fdata <- data[, c(1,top3Contrib)]
```





Time to look for multicolinearity:

If you see [Graph 02](#g02), the risk for muticolinearity (i.e. our predictor are not independent among each other) is high.


## Regression


### Goodness of fit


# Results



# Appendix


<a name="g01"></a>**Graph 1: Scatter matrix, all variables**

```{r, echo=FALSE, cache=TRUE}
print(pairsXY)

```

<a name="g02"></a>**Graph 2: Correlation matrix**

```{r, echo=FALSE}
print(corXY)

```

<a name="g03"></a>**Graph 3: Singular value decomposition, Variance explained**

```{r, dependson="svdChunk", echo=FALSE}
# graph for % of variation explained
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", 
     pch = 19)

```



